{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, regexp_replace, lower, col, explode, regexp_extract\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.orc('data/dataframe.orc')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://meta.stackexchange.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede\n",
    "df.select('Title', 'Body', 'Score', 'Tags', 'PostTypeId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only questions\n",
    "questions = df.filter(col('PostTypeId') == 1)\n",
    "questions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Body\", outputCol=\"words\")\n",
    "stopwords = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"stopwords\")\n",
    "punctuation = StopWordsRemover(inputCol=stopwords.getOutputCol(), outputCol=\"filtered\", stopWords=[''] + list(string.punctuation))\n",
    "\n",
    "stages = [tokenizer, stopwords, punctuation]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pipeline.fit(questions).transform(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_count = words.select(explode(col('filtered')).alias('word')).groupby('word').count().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_count.sort(col('count').desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 100000\n",
    "vocab = word_count.sort(col('count').desc()).limit(VOCAB_SIZE).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['count'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.iloc[3:1000]['count'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = dict(vocab.reset_index().set_index('word')['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.linalg import Vectors, SparseVector, DenseVector\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(words: list) -> list:\n",
    "    bag = np.zeros(VOCAB_SIZE, dtype=int)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in lookup:\n",
    "            bag[lookup[word]] += 1\n",
    "        \n",
    "    return bag.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkBag = T.ArrayType(T.LongType())\n",
    "\n",
    "count_vectorizer = F.udf(bag_of_words, SparkBag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = words.withColumn('vector', count_vectorizer(F.col('filtered')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized.select('Title', 'Body', 'words', 'stopwords', 'filtered', 'vector').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=punctuation.getOutputCol(), outputCol=\"counts\", minDF=2.0, vocabSize=50000)\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=stages + [cv, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pipeline.fit(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf.transform(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.select('Body', 'words', 'stopwords', 'filtered', 'counts', 'features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel = tfidf.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverseFreq = idfModel.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[inverseFreq.argsort()[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[inverseFreq.argsort()[-20:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-verse",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA, LocalLDAModel, DistributedLDAModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=30, featuresCol=\"counts\", seed=1, optimizer='online', maxIter=10, optimizeDocConcentration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lda.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LocalLDAModel.load('lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = model.logLikelihood(features)\n",
    "lp = model.logPerplexity(features)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.describeTopics(5)\n",
    "topics.show(truncate=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized = model.transform(features)\n",
    "categorized.select('Title', 'topicDistribution').show(truncate=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-syndicate",
   "metadata": {},
   "source": [
    "## Interpreting LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsDF = topics.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.getStages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, vectorizerModel, _ = tfidf.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerModel.vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizerModel.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsDF['words'] = topicsDF.termIndices.apply(lambda x: vocab[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-intensity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "prime-monster",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = [5, 20, 50]\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lda.k, num_topics) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVal = lda.fitMultiple(features, paramMaps = paramGrid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
